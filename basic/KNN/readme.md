我们需要有参数模型和有参数函数

有时候我们可以通过和一个顾客类似顾客的选择来为该顾客进行推荐，这也就是物以类聚的道理，我们可以通过一个有朋友周围好朋友的成绩来推测该小朋友的成绩。

## KNN 模型(K-Nearest Neighbor)
KNN 算法的一个最吸引人的特点是简单易懂，易于实现在零到很少的训练时间，可以成为一个有用的工具，对一些数据集的即时分析，这也是KNN 的优点。因为 KNN 不需要带**有参数**的模型进行训练，所以 KNN 是无参数的模型，可以用于分类和回归。这里值得注意 k 并不是模型参数而是我们事先指定的。KNN 也可以在时间序列上做拟合也就是回归

如何通过 KNN 来学习出一条拟合曲线，如果要预测某一个点值，就取其周围的 k 点然后求这点邻近点的均值作为该点估计值。
$$x_i = y_i$$
$$ y = \frac{\sum_{i=1}^k y_i}{K}$$
但是有时候发现这些邻近点距离要估计点距离也是不一样的，我们可以通过按距离(倒数)加权来求取平均来进行估计。
$$w_i \varpropto \frac{1}{d_i}$$

### 如何度量距离
其中 K 表示在新样本点附近(距离)选取 K 个样本数据，通过在 K 个样本进行投票来判断新增样本的类型。

- k 是否可以取偶数，当 K 为偶数时，如果 K 点不同类别样本点各占一半，我们不是无法对新样板点进行推测。我们可以通过距离来进一步推测样本点类别。
- 有关 k 系数的选择也是有一定技巧，k 取 1 可能发生过拟合，如果 K 选取无穷大，那么就变成哪个点多就把任何新的数据分为哪个数据多类别
- 任何两个点距离的之间连接连线，每一个小块，最近邻的数据，不再是中线而是面，
- k近邻可以做预测，估计一个人成绩，也就是我们可以通过临近点的均值或者带有按离推测样本点距离作为权重来计算该样本点的值。

#### A 和 B 两点间距离需要满足以下条件

- 对称 d(A,B) = d(B,A) 也就是 A 到 B 的距离等于 B 到 A 的距离
- d(A,A) = 0 样本点到自己本身的距离为 0 
- d(A,B) = 0 iff A = B 两点距离为 0 那么这两个点就是同一个点
- d(A,B) <= d(A,C) + d(B,C) 三角不等式

#### 距离(模)
- 欧氏距离
- 曼哈顿距离
- 切比雪夫距离
- hamming distance(异或运算)，通过两个二进制数做异或来得到估计值
- consine similarity
$$\cos(x,y) = \frac{x^Ty}{||x||_2 ||y||_2}$$