### Bagging
相对比较简单，但是其算法能力已经远超出我们想象。

bagging 是有放回地进行采样，在保证测试数据不变前提，对于训练数据集进行有放回采样$m_1,m_2,\dots,m=M$ 我们这样做目的何在?

上图中，我们原始数据集中分别用红和绿表示两种不同样本，每一次有

采样数据都，在新的数据有发生变换,可以消除一些噪声这样我们可以更真实得到数据，在这种情况下生产很多不同数据，每一组数据都会生成新的决策树，然后通过投票形式来决定未知样本所属类别。
$$F(x^{(i)}) = \frac{1}{K} \sum_{k=1}^K T_k (x^{(i)})$$
可以将决策树模型集成模型推广到任何弱分类上，依然有效
$$F(x^{(i)}) = \frac{1}{K} \sum_{k=1}^K M_k (x^{(i)})$$

### Boosting

$$D = {(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),\dots,(x^{(N)},y^{(N)})} x{(i)} \in \mathbb{R}^n$$

$$y \in {-1,+1}$$

#### 初始化
$$W = (w^{(1)},w^{(2)},\dots,w^{(N)})$$
学习到结果会对数据权重产生作用，从新调整权重，boosting 是完全不一样。bagging 可以并行处理，而 boosting 只能串行处理。
$$G_k(x) = $$
$$e_k = p(G_k(x)  $$

#### 分类器类别
- 随机分类器
$$\begin{cases}
    e_k = 0.5 \\
    \alpha_k = \frac{1}{2} \ln (\frac{0.5}{0.2}) = \frac{}{}
\end{cases}$$
- 可用的分类器
- 精准分类器
#### 计算误差
#### 更新数据权重
#### 组合分类器
$$f(x^{(i)}) = \sum_{k=1}^K \alpha_k G_k(x^{(i)})$$
$$G(x^{(i)}) = sign (f(x^{(i)}))$$
将每一个弱分类器的
$$$$