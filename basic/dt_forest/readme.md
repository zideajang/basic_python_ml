## 随机森林

### Bagging
相对比较简单，但是其算法能力已经远超出我们想象。

bagging 是有放回地进行采样，在保证测试数据不变前提，对于训练数据集进行有放回采样$m_1,m_2,\dots,m=M$ 我们这样做目的何在?

上图中，我们原始数据集中分别用红和绿表示两种不同样本，每一次进行随机采样都是对真实数据反映和写照。实际上我们生成 K 组的数据，那么我们做这件事的目的是

采样数据都，在新的数据有发生变换,可以消除一些噪声这样我们可以更真实得到数据，在这种情况下生产很多不同数据，每一组数据都会生成新的决策树，然后通过投票形式来决定未知样本所属类别。
$$F(x^{(i)}) = \frac{1}{K} \sum_{k=1}^K T_k (x^{(i)})$$
可以将决策树模型集成模型推广到任何弱分类上，依然有效，类似之前学过贝叶斯分类器也属于弱分类器
$$F(x^{(i)}) = \frac{1}{K} \sum_{k=1}^K M_k (x^{(i)})$$
我们是将每一个分类器的结果进行加和后求均值，那么我们就默认每一个分类器对结果的贡献都是相当的，也就是权重都为 1。实际这种情况我们再考虑下每个分类器都有其自身权重，甚至每一个数据都有自己权重。
### Boosting

$$D = {(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),\dots,(x^{(N)},y^{(N)})} x{(i)} \in \mathbb{R}^n$$

$$y \in {-1,+1}$$
有关数据我在这里不再啰嗦，大家到现在应该一眼就能看出我们样本

#### 初始化
对每一个权重进行初始进行赋值，无论在决策树还是在随后深度学习模型中给初始值还都有一些技巧，有时候我们给均值，有时候给正态分布等等，还是有一定技巧的。
$$W = (w^{(1)},w^{(2)},\dots,w^{(N)})$$
首先采集数据然后学习出一个决策树，学习到结果会对数据权重产生作用，改变数据权重，然后再学习第 2 个分类器，第 2 分类器在上面结果上，重新调整数据权重。依次学习到第 k 分类器，与 bagging 是完全不一样。bagging 可以并行处理，而 boosting 只能串行处理。
我们定义的弱分类器 $G_k(x) $ 这个分类器可以是决策树也可以是贝叶斯分类器。
$$e_k = p(G_k(x^{(i)}\neq y^{(i)})  $$
$$ e_k = \sum_i w^{(i)} I(G_k(x^{(i)})\neq y^{(i)})$$
这里I是计数函数
$$\begin{cases}
    I(True) = 1 \\
    I(False) = 0
\end{cases}$$

$$\alpha_k = \frac{1}{2} \ln \left( \frac{1 - e^k}{e^k} \right)$$
#### 分类器类别
- 随机分类器
$$\begin{cases}
    e_k = 0.5 \\
    \alpha_k = \frac{1}{2} \ln (\frac{0.5}{0.2}) = \frac{}{}
\end{cases}$$
- 可用的分类器
- 精准分类器
#### 计算误差
#### 更新数据权重
#### 组合分类器
$$f(x^{(i)}) = \sum_{k=1}^K \alpha_k G_k(x^{(i)})$$
$$G(x^{(i)}) = sign (f(x^{(i)}))$$
将每一个弱分类器的
$$$$